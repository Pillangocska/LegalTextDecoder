{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEouCu_Znntf",
        "outputId": "ffa78bab-63d4-4612-c402-762ef2fad723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BOW + MLP Overfitting Experiment\n",
            "Hungarian Terms and Conditions Readability Prediction\n",
            "============================================================\n",
            "\n",
            "[1] Loading data...\n",
            "Total samples in dataset: 2906\n",
            "Label distribution:\n",
            "label_numeric\n",
            "1    132\n",
            "2    318\n",
            "3    634\n",
            "4    925\n",
            "5    897\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Batch size: 32\n",
            "Batch label distribution:\n",
            "label_numeric\n",
            "1    7\n",
            "2    7\n",
            "3    7\n",
            "4    7\n",
            "5    4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[2] Creating BOW features...\n",
            "Vocabulary size: 500\n",
            "BOW shape: (32, 500)\n",
            "\n",
            "[3] Building model with hidden_dim=11...\n",
            "Model architecture: 500 -> 11 -> 5\n",
            "Total parameters: 5,571\n",
            "\n",
            "[4] Training for 1000 epochs...\n",
            "--------------------------------------------------\n",
            "Epoch    1 | Loss: 1.6449 | Accuracy: 21.9%\n",
            "Epoch  100 | Loss: 0.0061 | Accuracy: 100.0%\n",
            "Epoch  200 | Loss: 0.0019 | Accuracy: 100.0%\n",
            "Epoch  300 | Loss: 0.0010 | Accuracy: 100.0%\n",
            "Epoch  400 | Loss: 0.0006 | Accuracy: 100.0%\n",
            "Epoch  500 | Loss: 0.0004 | Accuracy: 100.0%\n",
            "Epoch  600 | Loss: 0.0003 | Accuracy: 100.0%\n",
            "Epoch  700 | Loss: 0.0002 | Accuracy: 100.0%\n",
            "Epoch  800 | Loss: 0.0002 | Accuracy: 100.0%\n",
            "Epoch  900 | Loss: 0.0001 | Accuracy: 100.0%\n",
            "Epoch 1000 | Loss: 0.0001 | Accuracy: 100.0%\n",
            "--------------------------------------------------\n",
            "\n",
            "[5] Final evaluation on training batch:\n",
            "Final accuracy: 100.0%\n",
            "\n",
            "Per-class performance:\n",
            "  Class 1: 100.0% (7 samples)\n",
            "  Class 2: 100.0% (7 samples)\n",
            "  Class 3: 100.0% (7 samples)\n",
            "  Class 4: 100.0% (7 samples)\n",
            "  Class 5: 100.0% (4 samples)\n",
            "\n",
            "============================================================\n",
            "✓ SUCCESS: Model successfully overfit on the batch!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "BOW + MLP overfitting experiment for Hungarian Terms and Conditions readability prediction.\n",
        "Goal: Overfit on a single batch (32 samples) to verify the model can learn.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Tuple, List, Optional\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "class HungarianBOWVectorizer:\n",
        "    \"\"\"Simple Bag-of-Words vectorizer for Hungarian text.\"\"\"\n",
        "\n",
        "    def __init__(self, max_features: int = 500, min_freq: int = 1) -> None:\n",
        "        self.max_features = max_features\n",
        "        self.min_freq = min_freq\n",
        "        self.vocab: dict[str, int] = {}\n",
        "        self.vocab_size: int = 0\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Simple whitespace + punctuation tokenizer.\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[^\\w\\sáéíóöőúüű]\", \" \", text)\n",
        "        tokens = text.split()\n",
        "        return [t for t in tokens if len(t) > 1]\n",
        "\n",
        "    def fit(self, texts: List[str]) -> \"HungarianBOWVectorizer\":\n",
        "        \"\"\"Build vocabulary from texts.\"\"\"\n",
        "        word_counts: Counter = Counter()\n",
        "        for text in texts:\n",
        "            tokens = self._tokenize(text)\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "        filtered_words = [\n",
        "            (word, count)\n",
        "            for word, count in word_counts.items()\n",
        "            if count >= self.min_freq\n",
        "        ]\n",
        "        sorted_words = sorted(filtered_words, key=lambda x: -x[1])\n",
        "        top_words = sorted_words[: self.max_features]\n",
        "\n",
        "        self.vocab = {word: idx for idx, (word, _) in enumerate(top_words)}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Transform texts to BOW vectors.\"\"\"\n",
        "        vectors = np.zeros((len(texts), self.vocab_size), dtype=np.float32)\n",
        "        for i, text in enumerate(texts):\n",
        "            tokens = self._tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token in self.vocab:\n",
        "                    vectors[i, self.vocab[token]] += 1\n",
        "        # L2 normalize\n",
        "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "        norms = np.where(norms == 0, 1, norms)\n",
        "        vectors = vectors / norms\n",
        "        return vectors\n",
        "\n",
        "    def fit_transform(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Fit and transform in one step.\"\"\"\n",
        "        self.fit(texts)\n",
        "        return self.transform(texts)\n",
        "\n",
        "\n",
        "class ReadabilityMLP(nn.Module):\n",
        "    \"\"\"Small MLP for readability classification (~6k parameters target).\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 12, num_classes: int = 5) -> None:\n",
        "        super().__init__()\n",
        "        # Architecture: input -> hidden -> output\n",
        "        # Parameters: input_dim * hidden_dim + hidden_dim (bias) + hidden_dim * num_classes + num_classes (bias)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    \"\"\"Count trainable parameters.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def calculate_architecture(vocab_size: int, target_params: int = 6000, num_classes: int = 5) -> int:\n",
        "    \"\"\"Calculate hidden dimension to achieve target parameter count.\n",
        "\n",
        "    Parameters = vocab_size * hidden + hidden + hidden * num_classes + num_classes\n",
        "               = hidden * (vocab_size + 1 + num_classes) + num_classes\n",
        "    \"\"\"\n",
        "    hidden = (target_params - num_classes) // (vocab_size + 1 + num_classes)\n",
        "    return max(4, hidden)\n",
        "\n",
        "\n",
        "def load_data(path: str, batch_size: int = 32) -> Tuple[List[str], np.ndarray]:\n",
        "    \"\"\"Load and sample data for overfitting experiment.\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    print(f\"Total samples in dataset: {len(df)}\")\n",
        "    print(f\"Label distribution:\\n{df['label_numeric'].value_counts().sort_index()}\")\n",
        "\n",
        "    # Sample a batch, trying to get some variety in labels\n",
        "    if len(df) > batch_size:\n",
        "        # Stratified-ish sampling: take some from each class if possible\n",
        "        sampled_indices: List[int] = []\n",
        "        for label in sorted(df[\"label_numeric\"].unique()):\n",
        "            label_df = df[df[\"label_numeric\"] == label]\n",
        "            n_samples = min(len(label_df), batch_size // 5 + 1)\n",
        "            sampled_indices.extend(label_df.sample(n=n_samples, random_state=42).index.tolist())\n",
        "        sampled_indices = sampled_indices[:batch_size]\n",
        "        df_batch = df.loc[sampled_indices]\n",
        "    else:\n",
        "        df_batch = df.head(batch_size)\n",
        "\n",
        "    print(f\"\\nBatch size: {len(df_batch)}\")\n",
        "    print(f\"Batch label distribution:\\n{df_batch['label_numeric'].value_counts().sort_index()}\")\n",
        "\n",
        "    texts = df_batch[\"text\"].tolist()\n",
        "    labels = df_batch[\"label_numeric\"].values - 1  # Convert 1-5 to 0-4\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "def train_overfit(\n",
        "    model: nn.Module,\n",
        "    X: torch.Tensor,\n",
        "    y: torch.Tensor,\n",
        "    epochs: int = 1000,\n",
        "    lr: float = 0.01,\n",
        "    print_every: int = 100,\n",
        ") -> List[float]:\n",
        "    \"\"\"Train model to overfit on a single batch.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    losses: List[float] = []\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
        "            with torch.no_grad():\n",
        "                predictions = outputs.argmax(dim=1)\n",
        "                accuracy = (predictions == y).float().mean().item()\n",
        "                print(f\"Epoch {epoch + 1:4d} | Loss: {loss.item():.4f} | Accuracy: {accuracy * 100:.1f}%\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"BOW + MLP Overfitting Experiment\")\n",
        "    print(\"Hungarian Terms and Conditions Readability Prediction\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Configuration\n",
        "    DATA_PATH = \"/content/train.csv\"\n",
        "    BATCH_SIZE = 32\n",
        "    TARGET_PARAMS = 6000\n",
        "    EPOCHS = 1000\n",
        "    LEARNING_RATE = 0.01\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\n[1] Loading data...\")\n",
        "    texts, labels = load_data(DATA_PATH, BATCH_SIZE)\n",
        "\n",
        "    # Create BOW features\n",
        "    print(\"\\n[2] Creating BOW features...\")\n",
        "    # Adjust max_features to control parameter count\n",
        "    # Start with a reasonable guess, then adjust\n",
        "    vectorizer = HungarianBOWVectorizer(max_features=500, min_freq=1)\n",
        "    X_bow = vectorizer.fit_transform(texts)\n",
        "    print(f\"Vocabulary size: {vectorizer.vocab_size}\")\n",
        "    print(f\"BOW shape: {X_bow.shape}\")\n",
        "\n",
        "    # Calculate hidden dim for target params\n",
        "    hidden_dim = calculate_architecture(vectorizer.vocab_size, TARGET_PARAMS)\n",
        "    print(f\"\\n[3] Building model with hidden_dim={hidden_dim}...\")\n",
        "\n",
        "    # Create model\n",
        "    model = ReadabilityMLP(input_dim=vectorizer.vocab_size, hidden_dim=hidden_dim, num_classes=5)\n",
        "    n_params = count_parameters(model)\n",
        "    print(f\"Model architecture: {vectorizer.vocab_size} -> {hidden_dim} -> 5\")\n",
        "    print(f\"Total parameters: {n_params:,}\")\n",
        "\n",
        "    # If too far from target, adjust vocab size\n",
        "    if abs(n_params - TARGET_PARAMS) > 1000:\n",
        "        print(f\"\\nAdjusting vocab size to get closer to {TARGET_PARAMS} params...\")\n",
        "        # Solve for vocab_size: params ≈ vocab_size * hidden + hidden * 6\n",
        "        # With hidden=12: params ≈ vocab_size * 12 + 72\n",
        "        target_vocab = (TARGET_PARAMS - 72) // 12 - 6\n",
        "        vectorizer = HungarianBOWVectorizer(max_features=target_vocab, min_freq=1)\n",
        "        X_bow = vectorizer.fit_transform(texts)\n",
        "\n",
        "        hidden_dim = calculate_architecture(vectorizer.vocab_size, TARGET_PARAMS)\n",
        "        model = ReadabilityMLP(input_dim=vectorizer.vocab_size, hidden_dim=hidden_dim, num_classes=5)\n",
        "        n_params = count_parameters(model)\n",
        "        print(f\"Adjusted vocabulary size: {vectorizer.vocab_size}\")\n",
        "        print(f\"Model architecture: {vectorizer.vocab_size} -> {hidden_dim} -> 5\")\n",
        "        print(f\"Total parameters: {n_params:,}\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_tensor = torch.tensor(X_bow, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Train\n",
        "    print(f\"\\n[4] Training for {EPOCHS} epochs...\")\n",
        "    print(\"-\" * 50)\n",
        "    losses = train_overfit(model, X_tensor, y_tensor, epochs=EPOCHS, lr=LEARNING_RATE)\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"-\" * 50)\n",
        "    print(\"\\n[5] Final evaluation on training batch:\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_tensor)\n",
        "        predictions = outputs.argmax(dim=1)\n",
        "        accuracy = (predictions == y_tensor).float().mean().item()\n",
        "        print(f\"Final accuracy: {accuracy * 100:.1f}%\")\n",
        "\n",
        "        # Per-class accuracy\n",
        "        print(\"\\nPer-class performance:\")\n",
        "        for cls in range(5):\n",
        "            mask = y_tensor == cls\n",
        "            if mask.sum() > 0:\n",
        "                cls_acc = (predictions[mask] == y_tensor[mask]).float().mean().item()\n",
        "                print(f\"  Class {cls + 1}: {cls_acc * 100:.1f}% ({mask.sum().item()} samples)\")\n",
        "            else:\n",
        "                print(f\"  Class {cls + 1}: N/A (0 samples)\")\n",
        "\n",
        "    # Check if overfit succeeded\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    if accuracy >= 0.95:\n",
        "        print(\"✓ SUCCESS: Model successfully overfit on the batch!\")\n",
        "    elif accuracy >= 0.7:\n",
        "        print(\"~ PARTIAL: Model learned something but didn't fully overfit.\")\n",
        "    else:\n",
        "        print(\"✗ FAILED: Model couldn't overfit on this batch.\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}