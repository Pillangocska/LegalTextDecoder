{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06dcfafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "PyTorch version: 2.9.1+cpu\n",
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c610b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING PREPROCESSED DATA\n",
      "============================================================\n",
      "✓ Loaded train_dataset: 2324 samples\n",
      "✓ Loaded val_dataset: 582 samples\n",
      "✓ Loaded test_dataset: 132 samples\n",
      "✓ Loaded class_weights\n",
      "✓ Loaded config\n",
      "\n",
      "Configuration:\n",
      "  Model: SZTAKI-HLT/hubert-base-cc\n",
      "  Max length: 256\n",
      "  Number of classes: 5\n",
      "\n",
      "Class weights:\n",
      "  Class 1: 4.4030\n",
      "  Class 2: 1.8277\n",
      "  Class 3: 0.9167\n",
      "  Class 4: 0.6283\n",
      "  Class 5: 0.6479\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load preprocessed datasets\n",
    "data_dir = Path('../_data/preprocessed')\n",
    "\n",
    "with open(data_dir / 'train_dataset.pkl', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "print(f\"✓ Loaded train_dataset: {len(train_dataset['labels'])} samples\")\n",
    "\n",
    "with open(data_dir / 'val_dataset.pkl', 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "print(f\"✓ Loaded val_dataset: {len(val_dataset['labels'])} samples\")\n",
    "\n",
    "with open(data_dir / 'test_dataset.pkl', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "print(f\"✓ Loaded test_dataset: {len(test_dataset['labels'])} samples\")\n",
    "\n",
    "with open(data_dir / 'class_weights.pkl', 'rb') as f:\n",
    "    class_weights = pickle.load(f)\n",
    "print(f\"✓ Loaded class_weights\")\n",
    "\n",
    "with open(data_dir / 'config.pkl', 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "print(f\"✓ Loaded config\")\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Model: {config['model_name']}\")\n",
    "print(f\"  Max length: {config['max_length']}\")\n",
    "print(f\"  Number of classes: {config['num_classes']}\")\n",
    "\n",
    "print(\"\\nClass weights:\")\n",
    "for cls, weight in class_weights.items():\n",
    "    print(f\"  Class {cls}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d229f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING PYTORCH DATASETS AND DATALOADERS\n",
      "============================================================\n",
      "✓ Created PyTorch datasets\n",
      "  Train: 2324 samples\n",
      "  Val:   582 samples\n",
      "  Test:  132 samples\n",
      "\n",
      "Hyperparameters:\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Number of epochs: 3\n",
      "\n",
      "✓ Created DataLoaders\n",
      "  Train batches: 146\n",
      "  Val batches:   37\n",
      "  Test batches:  9\n",
      "\n",
      "Sample batch verification:\n",
      "  input_ids shape: torch.Size([16, 256])\n",
      "  attention_mask shape: torch.Size([16, 256])\n",
      "  labels shape: torch.Size([16])\n",
      "  labels range: 0 to 4\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING PYTORCH DATASETS AND DATALOADERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class ASZFDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for ÁSZF readability classification\"\"\"\n",
    "\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['labels'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.encodings['labels'][idx] - 1, dtype=torch.long)  # Convert to 0-4\n",
    "        }\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_torch_dataset = ASZFDataset(train_dataset)\n",
    "val_torch_dataset = ASZFDataset(val_dataset)\n",
    "test_torch_dataset = ASZFDataset(test_dataset)\n",
    "\n",
    "print(f\"✓ Created PyTorch datasets\")\n",
    "print(f\"  Train: {len(train_torch_dataset)} samples\")\n",
    "print(f\"  Val:   {len(val_torch_dataset)} samples\")\n",
    "print(f\"  Test:  {len(test_torch_dataset)} samples\")\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_torch_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_torch_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Created DataLoaders\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# Verify sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch verification:\")\n",
    "print(f\"  input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"  labels shape: {sample_batch['labels'].shape}\")\n",
    "print(f\"  labels range: {sample_batch['labels'].min().item()} to {sample_batch['labels'].max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02c1b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIALIZING MODEL AND TRAINING COMPONENTS\n",
      "============================================================\n",
      "\n",
      "Loading model: SZTAKI-HLT/hubert-base-cc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SZTAKI-HLT/hubert-base-cc and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded and moved to cpu\n",
      "  Total parameters: 110,621,957\n",
      "  Trainable parameters: 110,621,957\n",
      "\n",
      "✓ Class weights prepared: [np.float64(4.403030303030303), np.float64(1.8276729559748428), np.float64(0.9167192429022082), np.float64(0.6283243243243243), np.float64(0.6479375696767001)]\n",
      "✓ Loss function: CrossEntropyLoss with class weights\n",
      "✓ Optimizer: AdamW (lr=2e-05)\n",
      "✓ Scheduler: Linear with warmup\n",
      "  Total training steps: 438\n",
      "  Warmup steps: 43\n",
      "\n",
      "✓ Log file: ..\\logs\\training_log_20251205_182730.txt\n",
      "\n",
      "✓ All training components initialized!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"INITIALIZING MODEL AND TRAINING COMPONENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load model\n",
    "model_name = config['model_name']\n",
    "num_labels = config['num_classes']\n",
    "\n",
    "print(f\"\\nLoading model: {model_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"✓ Model loaded and moved to {device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Prepare class weights tensor\n",
    "class_weights_list = [class_weights[i] for i in range(1, 6)]  # Classes 1-5\n",
    "class_weights_tensor = torch.tensor(class_weights_list, dtype=torch.float).to(device)\n",
    "print(f\"\\n✓ Class weights prepared: {class_weights_list}\")\n",
    "\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "print(f\"✓ Loss function: CrossEntropyLoss with class weights\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "print(f\"✓ Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),  # 10% warmup\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "print(f\"✓ Scheduler: Linear with warmup\")\n",
    "print(f\"  Total training steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {int(0.1 * total_steps)}\")\n",
    "\n",
    "# Create logging directory\n",
    "log_dir = Path('../logs')\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_file = log_dir / f'training_log_{timestamp}.txt'\n",
    "\n",
    "print(f\"\\n✓ Log file: {log_file}\")\n",
    "\n",
    "# Initialize log file\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"TRAINING LOG\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Model: {model_name}\\n\")\n",
    "    f.write(f\"Device: {device}\\n\")\n",
    "    f.write(f\"Batch size: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"Learning rate: {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"Epochs: {NUM_EPOCHS}\\n\")\n",
    "    f.write(f\"Total steps: {total_steps}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "print(f\"\\n✓ All training components initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0729c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEFINING TRAINING AND EVALUATION FUNCTIONS\n",
      "============================================================\n",
      "✓ Training and evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEFINING TRAINING AND EVALUATION FUNCTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch, log_file):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct/total:.4f}'\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Log to file\n",
    "    log_msg = f\"Epoch {epoch} - Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.4f}\\n\"\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(log_msg)\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, split_name, log_file):\n",
    "    \"\"\"Evaluate model on validation or test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f'Evaluating {split_name}'):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Log to file\n",
    "    log_msg = f\"{split_name} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}\\n\"\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(log_msg)\n",
    "\n",
    "    return avg_loss, accuracy, np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "print(\"✓ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe9af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Training for 3 epochs...\n",
      "This will take a while on CPU. Estimated time: ~30-60 min per epoch\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  22%|██▏       | 32/146 [12:31<44:36, 23.48s/it, loss=1.5823, acc=0.2305]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, scheduler, criterion, device, epoch, log_file)\u001b[39m\n\u001b[32m     26\u001b[39m loss = criterion(logits, labels)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m optimizer.step()\n\u001b[32m     31\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andras.janko\\Documents\\LegalTextDecoder\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andras.janko\\Documents\\LegalTextDecoder\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andras.janko\\Documents\\LegalTextDecoder\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Track best model\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"This will take a while on CPU. Estimated time: ~30-60 min per epoch\\n\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device, epoch, log_file\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc, val_preds, val_labels = evaluate(\n",
    "        model, val_loader, criterion, device, f\"Validation (Epoch {epoch})\", log_file\n",
    "    )\n",
    "\n",
    "    print(f\"Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.4f}\")\n",
    "\n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "        # Save model\n",
    "        model_dir = Path('../models')\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        best_model_path = model_dir / 'best_model.pt'\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "        }, best_model_path)\n",
    "\n",
    "        print(f\"✓ New best model saved! (Val Acc: {val_acc:.4f})\")\n",
    "\n",
    "        # Log to file\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"*** Best model saved at epoch {epoch} (Val Acc: {val_acc:.4f}) ***\\n\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nBest validation accuracy: {best_val_accuracy:.4f} (Epoch {best_epoch})\")\n",
    "\n",
    "# Log final results\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"TRAINING COMPLETE\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Best validation accuracy: {best_val_accuracy:.4f} (Epoch {best_epoch})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legaltextdecoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
